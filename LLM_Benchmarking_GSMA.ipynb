{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01403b2-4bca-4c46-8c54-2725f7c669ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f523c300-7a5a-4a8d-a6a8-92864026d7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and logging configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Configure logging to see informative messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "print(\"Imports and logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cae7275-e622-4063-b15b-2ba0c31aa32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Configuration ---\n",
    "\n",
    "# This config is for the INFERENCE step (generating answers)\n",
    "config = {\n",
    "  \"model_to_be_tested\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "  \"dataset_hf_name\": \"squad\",\n",
    "  \"judge_model\":\"google/gemma-3-1b-it\",\n",
    "  \"dataset_split\": \"validation\",\n",
    "  \"question_column_name\": \"question\",\n",
    "  \"prompt_template\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an expert assistant who provides concise and accurate answers.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please answer the following question: <question>\"\n",
    "    }\n",
    "  ],\n",
    "  \"num_samples\": 3,\n",
    "  \"max_new_tokens\": 1000,\n",
    "  \"output_file\": \"inference_results.json\"\n",
    "}\n",
    "\n",
    "print(\"Inference configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63155fe7-d27c-496a-9f10-9bca86321211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Helper Functions ---\n",
    "\n",
    "def load_hf_dataset(name: str, split: str, question_column: str) -> Dataset:\n",
    "    \"\"\"Loads a dataset from Hugging Face Hub and verifies the question column.\"\"\"\n",
    "    logging.info(f\"Loading dataset '{name}' (split: {split})...\")\n",
    "    try:\n",
    "        dataset = load_dataset(name, split=split)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset '{name}': {e}\")\n",
    "\n",
    "    if question_column not in dataset.column_names:\n",
    "        raise ValueError(\n",
    "            f\"Dataset '{name}' does not have a '{question_column}' column. \"\n",
    "            f\"Available columns: {dataset.column_names}\"\n",
    "        )\n",
    "    logging.info(f\"Dataset loaded. Using '{question_column}' as the question column.\")\n",
    "    return dataset\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Loads a causal language model and its tokenizer from Hugging Face Hub.\"\"\"\n",
    "    logging.info(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        logging.info(f\"Model loaded successfully on device: {model.device}\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load model or tokenizer '{model_name}': {e}\")\n",
    "\n",
    "def format_prompt(item: Dict, question_column: str, prompt_template: Any, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Formats the prompt using the provided template and data item.\"\"\"\n",
    "    try:\n",
    "        question = item[question_column]\n",
    "        if isinstance(prompt_template, str):\n",
    "            return prompt_template.replace(\"<question>\", question)\n",
    "        elif isinstance(prompt_template, list):\n",
    "            messages = [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"].replace(\"<question>\", question)}\n",
    "                for msg in prompt_template\n",
    "            ]\n",
    "            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported prompt_template type: {type(prompt_template)}\")\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Question column '{question_column}' not found in the data item.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error formatting prompt: {e}\")\n",
    "\n",
    "def perform_inference(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, max_new_tokens: int) -> Tuple[List[int], int]:\n",
    "    \"\"\"Performs inference using the model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids_len = inputs.input_ids.shape[1]\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        return generated_ids[0].tolist(), input_ids_len\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logging.error(\"CUDA Out of Memory. Try a smaller model, fewer samples, or quantization.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during model generation: {e}\")\n",
    "        raise e\n",
    "\n",
    "def parse_generation(tokenizer: PreTrainedTokenizer, full_output_ids: List[int], input_length: int) -> str:\n",
    "    \"\"\"Decodes the generated token IDs, excluding the prompt.\"\"\"\n",
    "    output_ids = full_output_ids[input_length:]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "print(\"Inference helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837798cb-600e-4582-8566-e6f804c48f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 11:11:33 - INFO - Loading model and tokenizer for 'Qwen/Qwen2-0.5B-Instruct'...\n",
      "2025-06-17 11:11:34 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-06-17 11:11:34 - INFO - Model loaded successfully on device: cuda:0\n",
      "2025-06-17 11:11:34 - INFO - Loading dataset 'squad' (split: validation)...\n",
      "2025-06-17 11:11:35 - INFO - Dataset loaded. Using 'question' as the question column.\n",
      "2025-06-17 11:11:35 - INFO - --- Processing sample 1/3 ---\n",
      "2025-06-17 11:11:36 - INFO - Generated response for sample 1.\n",
      "2025-06-17 11:11:36 - INFO - --- Processing sample 2/3 ---\n",
      "2025-06-17 11:11:36 - INFO - Generated response for sample 2.\n",
      "2025-06-17 11:11:36 - INFO - --- Processing sample 3/3 ---\n",
      "2025-06-17 11:11:37 - INFO - Generated response for sample 3.\n",
      "2025-06-17 11:11:37 - INFO - Saving 3 results to inference_results.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference completed successfully! Results saved to inference_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- Inference Pipeline ---\n",
    "\n",
    "results = []\n",
    "try:\n",
    "    # Unpack configuration\n",
    "    model_name = config[\"model_to_be_tested\"]\n",
    "    dataset_name = config[\"dataset_hf_name\"]\n",
    "    dataset_split = config[\"dataset_split\"]\n",
    "    question_column = config[\"question_column_name\"]\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "    num_samples = config[\"num_samples\"]\n",
    "    max_new_tokens = config[\"max_new_tokens\"]\n",
    "    output_file = Path(config[\"output_file\"])\n",
    "\n",
    "    # Load Model and Dataset\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    dataset = load_hf_dataset(dataset_name, dataset_split, question_column)\n",
    "\n",
    "    # Process Samples\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        logging.info(f\"--- Processing sample {i+1}/{num_samples} ---\")\n",
    "        item = dataset[i]\n",
    "        \n",
    "        prompt = format_prompt(item, question_column, prompt_template, tokenizer)\n",
    "        generated_ids, input_len = perform_inference(model, tokenizer, prompt, max_new_tokens)\n",
    "        llm_response = parse_generation(tokenizer, generated_ids, input_len)\n",
    "        \n",
    "        ground_truth = item.get(\"answers\", {}).get(\"text\", [\"N/A\"])[0]\n",
    "\n",
    "        # SIMPLIFICATION: Save with the key 'ground_truth' to avoid renaming it later.\n",
    "        results.append({\n",
    "            \"question\": item[question_column],\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"llm_response\": llm_response,\n",
    "        })\n",
    "        logging.info(f\"Generated response for sample {i+1}.\")\n",
    "\n",
    "    # Save Results\n",
    "    logging.info(f\"Saving {len(results)} results to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nInference completed successfully! Results saved to {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during inference: {e}\", exc_info=True)\n",
    "    print(f\"\\nInference pipeline failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aab39a3-d1cd-4aa3-81e9-84896d5ef4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Simplified Evaluation Functions ---\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "from transformers import pipeline\n",
    "\n",
    "class SimpleScoreParser(BaseOutputParser):\n",
    "    \"\"\"A simple parser to extract a single score from the judge's response.\"\"\"\n",
    "    def parse(self, text: str) -> dict:\n",
    "        \"\"\"Parses the LLM output to extract the first valid JSON object.\"\"\"\n",
    "        try:\n",
    "            # Find the start of the first JSON object\n",
    "            start_idx = text.find('{')\n",
    "            if start_idx == -1:\n",
    "                raise ValueError(\"No JSON object found in the response.\")\n",
    "\n",
    "            # Find the corresponding closing brace for the first JSON object\n",
    "            open_braces = 0\n",
    "            for i in range(start_idx, len(text)):\n",
    "                if text[i] == '{':\n",
    "                    open_braces += 1\n",
    "                elif text[i] == '}':\n",
    "                    open_braces -= 1\n",
    "                    if open_braces == 0:\n",
    "                        end_idx = i + 1\n",
    "                        break\n",
    "            else: # This 'else' belongs to the 'for' loop\n",
    "                raise ValueError(\"Could not find a complete JSON object.\")\n",
    "\n",
    "            # Extract and parse the identified JSON string\n",
    "            json_str = text[start_idx:end_idx]\n",
    "            data = json.loads(json_str)\n",
    "\n",
    "            # Ensure required keys exist, default to safe values\n",
    "            score = float(data.get(\"overall_score\", 0.0))\n",
    "            explanation = data.get(\"explanation\", \"No explanation provided\")\n",
    "            return {\"overall_score\": score, \"explanation\": explanation}\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError, AttributeError, NameError) as e:\n",
    "            # NameError can happen if end_idx is not assigned\n",
    "            logging.warning(f\"Could not parse score from response: {text}\\nError: {e}\")\n",
    "            return {\"overall_score\": 0.0, \"explanation\": f\"Parsing failed: {str(e)}\"}\n",
    "\n",
    "# A much simpler and stricter prompt for the judge model\n",
    "simple_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"ground_truth\", \"llm_response\"],\n",
    "    template=\"\"\"You are a strict evaluator. Your only job is to provide a single score and a detailed explanation for the score.\n",
    "Based on the Ground Truth, score the LLM Response from 0.0 (completely wrong) to 10.0 (perfectly correct).\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "LLM Response: {llm_response}\n",
    "\n",
    "You MUST ONLY output a JSON object with exactly two keys: \"overall_score\" and \"explanation\". Do not add any other text or code blocks.\n",
    "Example JSON output: {{\"overall_score\": 8.5, \"explanation\": \"The response is accurate but lacks some details present in the ground truth.\"}}\n",
    "\n",
    "JSON output:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def setup_hf_evaluator(judge_model_name: str, hf_token: str = None):\n",
    "    \"\"\"Sets up the Hugging Face pipeline for the evaluator model.\"\"\"\n",
    "    logging.info(f\"Setting up judge model: {judge_model_name}\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    try:\n",
    "        return pipeline(\"text-generation\", model=judge_model_name, device=device, token=hf_token)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load judge model '{judge_model_name}': {e}\")\n",
    "\n",
    "def batch_evaluate_simple_score(evaluation_data: list, judge_model_name: str, hf_token: str = None):\n",
    "    \"\"\"Evaluates responses to get a single score and explanation for each.\"\"\"\n",
    "    if not evaluation_data:\n",
    "        return []\n",
    "\n",
    "    evaluator = setup_hf_evaluator(judge_model_name, hf_token)\n",
    "    parser = SimpleScoreParser()\n",
    "    final_results = []\n",
    "\n",
    "    for i, item in enumerate(evaluation_data):\n",
    "        logging.info(f\"--- Evaluating item {i+1}/{len(evaluation_data)} ---\")\n",
    "        prompt = simple_evaluation_prompt.format(\n",
    "            ground_truth=item['ground_truth'],\n",
    "            llm_response=item['llm_response']\n",
    "        )\n",
    "        \n",
    "        raw_response = evaluator(prompt, max_new_tokens=200, return_full_text=False)[0]['generated_text']\n",
    "        # Parse the response using SimpleScoreParser\n",
    "        parsed_response = parser.parse(raw_response)\n",
    "        \n",
    "        # Construct the final dictionary\n",
    "        final_results.append({\n",
    "            \"question\": item['question'],\n",
    "            \"ground_truth\": item['ground_truth'],\n",
    "            \"llm_response\": item['llm_response'],\n",
    "            \"final_score\": parsed_response[\"overall_score\"],\n",
    "            \"judge_raw_response\": parsed_response[\"explanation\"],\n",
    "        })\n",
    "            \n",
    "    return final_results\n",
    "\n",
    "print(\"Simplified evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "377a0bd6-c100-4e49-b785-70a24c5aae72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 11:11:41 - INFO - Setting up judge model: google/gemma-3-1b-it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting simplified batch evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "2025-06-17 11:11:44 - INFO - --- Evaluating item 1/3 ---\n",
      "2025-06-17 11:12:11 - INFO - --- Evaluating item 2/3 ---\n",
      "2025-06-17 11:12:30 - INFO - --- Evaluating item 3/3 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete. Results saved to final_scores.json\n",
      "\n",
      "--- FINAL SCORES ---\n",
      "[\n",
      "  {\n",
      "    \"question\": \"Which NFL team represented the AFC at Super Bowl 50?\",\n",
      "    \"ground_truth\": \"Denver Broncos\",\n",
      "    \"llm_response\": \"The New England Patriots represented the AFC at Super Bowl 50.\",\n",
      "    \"final_score\": 2.0,\n",
      "    \"judge_raw_response\": \"The response is incorrect and fails to accurately represent the Denver Broncos.\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Which NFL team represented the NFC at Super Bowl 50?\",\n",
      "    \"ground_truth\": \"Carolina Panthers\",\n",
      "    \"llm_response\": \"The New Orleans Saints represented the NFC at Super Bowl 50. They played their final game against the Pittsburgh Steelers in Super Bowl LII, where they won by a score of 34-28.\",\n",
      "    \"final_score\": 9.0,\n",
      "    \"judge_raw_response\": \"The response accurately states the Carolina Panthers' representation at Super Bowl 50, the opposing team, and the outcome of the game. It is a solid and direct answer.\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Where did Super Bowl 50 take place?\",\n",
      "    \"ground_truth\": \"Santa Clara, California\",\n",
      "    \"llm_response\": \"Super Bowl 50 was held at the Mercedes-Benz Stadium in Atlanta, Georgia, United States. It took place on February 7, 2016. The game was played between the New England Patriots and the Kansas City Chiefs.\",\n",
      "    \"final_score\": 8.0,\n",
      "    \"judge_raw_response\": \"The response is accurate and provides the core information about Super Bowl 50's location and the participating teams. However, it misses a crucial detail - the specific date of the game.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# --- Final Execution Pipeline ---\n",
    "\n",
    "# 1. Define configuration for the JUDGE model\n",
    "judge_model_name = config[\"judge_model\"]\n",
    "hf_token = \"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\"  # IMPORTANT: Replace with your actual Hugging Face token\n",
    "\n",
    "# 2. Load the results from the inference step\n",
    "def load_json_from_file(file_path: str) -> Union[Dict, List, Any]:\n",
    "    \"\"\"Safely loads data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "inference_results_file = config.get(\"output_file\", \"inference_results.json\")\n",
    "data_to_evaluate = load_json_from_file(inference_results_file)\n",
    "\n",
    "# 3. Run the simplified batch evaluation\n",
    "if data_to_evaluate:\n",
    "    print(\"\\nStarting simplified batch evaluation...\")\n",
    "    final_scores = batch_evaluate_simple_score(\n",
    "        evaluation_data=data_to_evaluate,\n",
    "        judge_model_name=judge_model_name,\n",
    "        hf_token=hf_token\n",
    "    )\n",
    "\n",
    "    # 4. Save and print the final, simple results\n",
    "    evaluation_output_file = \"final_scores.json\"\n",
    "    with open(evaluation_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. Results saved to {evaluation_output_file}\")\n",
    "    print(\"\\n--- FINAL SCORES ---\")\n",
    "    print(json.dumps(final_scores, indent=2))\n",
    "else:\n",
    "    print(\"\\nCould not proceed with evaluation. No data loaded from inference file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69c8b2-2860-45c7-bc66-f0c4e8ebfb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
