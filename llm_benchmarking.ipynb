{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402716f5",
   "metadata": {},
   "source": [
    "## LLM Response Generation and Evaluation Framework\n",
    "<img src=\"https://raw.githubusercontent.com/NetoAI/Netoai-Llm-Eval/main/netoai_logo.png\" alt=\"NetoAI Logo\" width=\"300\"/>\n",
    "\n",
    "\n",
    "This section demonstrates how to generate responses using a selected LLM and evaluate them with a judge model. The input dataset must contain `question` and `answer` columns. The prompt template should include a `<question>` placeholder, which will be dynamically replaced during inference. The generated output is stored in a structured format and later evaluated to assign numerical scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cfca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Configure logging to see informative messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "print(\"Imports and logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc55fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Inference Configuration ---\n",
    "\n",
    "# This config is for the INFERENCE step (generating answers)\n",
    "config = {\n",
    "  \"model_to_be_tested\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "  \"dataset_hf_name\": \"squad\",\n",
    "  \"judge_model\":\"microsoft/Phi-4-mini-instruct\",\n",
    "  \"dataset_split\": \"validation\",\n",
    "  \"question_column_name\": \"question\",\n",
    "  \"prompt_template\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an expert assistant who provides concise and accurate answers.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please answer the following question: <question>\"\n",
    "    }\n",
    "  ],\n",
    "  \"num_samples\": 3,\n",
    "  \"max_new_tokens\": 1000,\n",
    "  \"output_file\": \"inference_results.json\"\n",
    "}\n",
    "\n",
    "print(\"Inference configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Helper Functions ---\n",
    "\n",
    "def load_hf_dataset(name: str, split: str, question_column: str) -> Dataset:\n",
    "    \"\"\"Loads a dataset from Hugging Face Hub and verifies the question column.\"\"\"\n",
    "    logging.info(f\"Loading dataset '{name}' (split: {split})...\")\n",
    "    try:\n",
    "        dataset = load_dataset(name, split=split)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset '{name}': {e}\")\n",
    "\n",
    "    if question_column not in dataset.column_names:\n",
    "        raise ValueError(\n",
    "            f\"Dataset '{name}' does not have a '{question_column}' column. \"\n",
    "            f\"Available columns: {dataset.column_names}\"\n",
    "        )\n",
    "    logging.info(f\"Dataset loaded. Using '{question_column}' as the question column.\")\n",
    "    return dataset\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Loads a causal language model and its tokenizer from Hugging Face Hub.\"\"\"\n",
    "    logging.info(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        logging.info(f\"Model loaded successfully on device: {model.device}\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load model or tokenizer '{model_name}': {e}\")\n",
    "\n",
    "def format_prompt(item: Dict, question_column: str, prompt_template: Any, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Formats the prompt using the provided template and data item.\"\"\"\n",
    "    try:\n",
    "        question = item[question_column]\n",
    "        if isinstance(prompt_template, str):\n",
    "            return prompt_template.replace(\"<question>\", question)\n",
    "        elif isinstance(prompt_template, list):\n",
    "            messages = [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"].replace(\"<question>\", question)}\n",
    "                for msg in prompt_template\n",
    "            ]\n",
    "            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported prompt_template type: {type(prompt_template)}\")\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Question column '{question_column}' not found in the data item.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error formatting prompt: {e}\")\n",
    "\n",
    "def perform_inference(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, max_new_tokens: int) -> Tuple[List[int], int]:\n",
    "    \"\"\"Performs inference using the model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids_len = inputs.input_ids.shape[1]\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        return generated_ids[0].tolist(), input_ids_len\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logging.error(\"CUDA Out of Memory. Try a smaller model, fewer samples, or quantization.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during model generation: {e}\")\n",
    "        raise e\n",
    "\n",
    "def parse_generation(tokenizer: PreTrainedTokenizer, full_output_ids: List[int], input_length: int) -> str:\n",
    "    \"\"\"Decodes the generated token IDs, excluding the prompt.\"\"\"\n",
    "    output_ids = full_output_ids[input_length:]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "print(\"Inference helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Pipeline ---\n",
    "\n",
    "results = []\n",
    "try:\n",
    "    # Unpack configuration\n",
    "    model_name = config[\"model_to_be_tested\"]\n",
    "    dataset_name = config[\"dataset_hf_name\"]\n",
    "    dataset_split = config[\"dataset_split\"]\n",
    "    question_column = config[\"question_column_name\"]\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "    num_samples = config[\"num_samples\"]\n",
    "    max_new_tokens = config[\"max_new_tokens\"]\n",
    "    output_file = Path(config[\"output_file\"])\n",
    "\n",
    "    # Load Model and Dataset\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    dataset = load_hf_dataset(dataset_name, dataset_split, question_column)\n",
    "\n",
    "    # Process Samples\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        logging.info(f\"--- Processing sample {i+1}/{num_samples} ---\")\n",
    "        item = dataset[i]\n",
    "        \n",
    "        prompt = format_prompt(item, question_column, prompt_template, tokenizer)\n",
    "        generated_ids, input_len = perform_inference(model, tokenizer, prompt, max_new_tokens)\n",
    "        llm_response = parse_generation(tokenizer, generated_ids, input_len)\n",
    "        \n",
    "        ground_truth = item.get(\"answers\", {}).get(\"text\", [\"N/A\"])[0]\n",
    "\n",
    "        # SIMPLIFICATION: Save with the key 'ground_truth' to avoid renaming it later.\n",
    "        results.append({\n",
    "            \"question\": item[question_column],\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"llm_response\": llm_response,\n",
    "        })\n",
    "        logging.info(f\"Generated response for sample {i+1}.\")\n",
    "\n",
    "    del model\n",
    "    del tokenizer\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    logging.info(\"Model and tokenizer unloaded from GPU, CUDA cache cleared.\")\n",
    "    \n",
    "    # Save Results\n",
    "    logging.info(f\"Saving {len(results)} results to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nInference completed successfully! Results saved to {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during inference: {e}\", exc_info=True)\n",
    "    print(f\"\\nInference pipeline failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parser for structured evaluation output\"\"\"\n",
    "    \n",
    "    def parse(self, text: str) -> Dict[str, Any]:\n",
    "        try:\n",
    "            # Extract JSON from the response\n",
    "            start_idx = text.find('{')\n",
    "            end_idx = text.rfind('}') + 1\n",
    "            json_str = text[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            # Fallback parsing if JSON extraction fails\n",
    "            return {\n",
    "                \"overall_score\": 0.0,\n",
    "                \"accuracy\": 0.0,\n",
    "                \"completeness\": 0.0,\n",
    "                \"relevance\": 0.0,\n",
    "                \"clarity\": 0.0,\n",
    "                \"reasoning\": \"Failed to parse evaluation\",\n",
    "                \"specific_issues\": [],\n",
    "                \"strengths\": []\n",
    "            }\n",
    "\n",
    "# Main evaluation prompt template (unchanged)\n",
    "evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"ground_truth\", \"llm_response\"],\n",
    "    template=\"\"\"\n",
    "You are an expert evaluator tasked with assessing the quality of an LLM response against ground truth data.\n",
    "\n",
    "**EVALUATION TASK:**\n",
    "Question: {question}\n",
    "\n",
    "Ground Truth Answer: {ground_truth}\n",
    "\n",
    "LLM Response to Evaluate: {llm_response}\n",
    "\n",
    "**EVALUATION CRITERIA:**\n",
    "Please evaluate the LLM response across the following dimensions (scale 0-10):\n",
    "\n",
    "1. **Accuracy**: How factually correct is the response compared to ground truth?\n",
    "2. **Completeness**: Does the response cover all key points from the ground truth?\n",
    "3. **Relevance**: How well does the response address the specific question asked?\n",
    "4. **Clarity**: Is the response clear, well-structured, and easy to understand?\n",
    "\n",
    "**EVALUATION INSTRUCTIONS:**\n",
    "1. Compare the LLM response directly with the ground truth\n",
    "2. Identify any factual errors, omissions, or additions\n",
    "3. Consider both content accuracy and presentation quality\n",
    "4. Provide specific examples for your scoring decisions\n",
    "\n",
    "**OUTPUT FORMAT:**\n",
    "Provide ONLY your evaluation as a JSON object with the following structure:\n",
    "\n",
    "{{\n",
    "    \"overall_score\": <float between 0-10>,\n",
    "    \"accuracy\": <float between 0-10>,\n",
    "    \"completeness\": <float between 0-10>,\n",
    "    \"relevance\": <float between 0-10>,\n",
    "    \"clarity\": <float between 0-10>,\n",
    "    \"reasoning\": \"<detailed explanation of your evaluation>\",\n",
    "    \"specific_issues\": [\"<list of specific problems found>\"],\n",
    "    \"strengths\": [\"<list of response strengths>\"],\n",
    "    \"factual_errors\": [\"<list of any factual errors>\"],\n",
    "    \"missing_information\": [\"<list of key information missing from ground truth>\"],\n",
    "    \"additional_information\": [\"<list of relevant info added beyond ground truth>\"]\n",
    "}}\n",
    "\n",
    "Begin your evaluation:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Alternative detailed evaluation prompt (unchanged)\n",
    "detailed_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"ground_truth\", \"llm_response\", \"evaluation_context\"],\n",
    "    template=\"\"\"\n",
    "You are conducting a comprehensive evaluation of an LLM response against established ground truth.\n",
    "\n",
    "**CONTEXT:** {evaluation_context}\n",
    "\n",
    "**QUESTION:** {question}\n",
    "\n",
    "**GROUND TRUTH:** {ground_truth}\n",
    "\n",
    "**LLM RESPONSE:** {llm_response}\n",
    "\n",
    "**DETAILED EVALUATION FRAMEWORK:**\n",
    "...\n",
    "\n",
    "**OUTPUT:**\n",
    "Provide ONLY a structured JSON evaluation with scores, detailed reasoning, and actionable feedback:\n",
    "\n",
    "{{\n",
    "    \"scores\": {{\n",
    "        \"overall\": <0-10>,\n",
    "        \"accuracy\": <0-10>,\n",
    "        \"completeness\": <0-10>,\n",
    "        \"relevance\": <0-10>,\n",
    "        \"clarity\": <0-10>,\n",
    "        \"organization\": <0-10>\n",
    "    }},\n",
    "    \"analysis\": {{\n",
    "        \"key_strengths\": [\"strength1\", \"strength2\"],\n",
    "        \"critical_weaknesses\": [\"weakness1\", \"weakness2\"],\n",
    "        \"factual_errors\": [\"error1\", \"error2\"],\n",
    "        \"missing_key_points\": [\"point1\", \"point2\"],\n",
    "        \"added_value\": [\"addition1\", \"addition2\"]\n",
    "    }},\n",
    "    \"detailed_feedback\": \"<comprehensive explanation of evaluation>\",\n",
    "    \"improvement_suggestions\": [\"suggestion1\", \"suggestion2\"],\n",
    "    \"confidence_level\": \"<high/medium/low> - your confidence in this evaluation\"\n",
    "}}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Parse JSON configuration from string\n",
    "def load_config_from_string(config_json: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        return json.loads(config_json)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Invalid JSON string provided: {str(e)}\")\n",
    "\n",
    "# Hugging Face Evaluator Setup with device-agnostic support\n",
    "def setup_hf_evaluator(config_json: str, hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Setup Hugging Face model as the evaluator LLM based on JSON string config, using CUDA if available, else CPU\n",
    "    \n",
    "    Args:\n",
    "        config_json: JSON string containing judge_model\n",
    "        hf_token: Hugging Face API token (if None, will use HF_TOKEN env var)\n",
    "    \n",
    "    Returns:\n",
    "        Hugging Face pipeline for text generation\n",
    "    \"\"\"\n",
    "    # Determine device\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        torch_dtype = torch.float16  # Use FP16 for CUDA\n",
    "        print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "    elif torch.backends.mps.is_available():  # For Apple silicon\n",
    "        device = \"mps\"\n",
    "        torch_dtype = torch.float16\n",
    "        print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        torch_dtype = torch.float32  # Use FP32 for CPU\n",
    "        print(\"Using CPU device\")\n",
    "\n",
    "    # Load configuration from JSON string\n",
    "    config = load_config_from_string(config_json)\n",
    "    judge_model = config.get(\"judge_model\")\n",
    "    if not judge_model:\n",
    "        raise ValueError(\"Judge model not specified in the JSON configuration.\")\n",
    "    \n",
    "    # Get Hugging Face token\n",
    "    if hf_token is None:\n",
    "        hf_token = os.getenv(\"HF_TOKEN\")\n",
    "        if not hf_token:\n",
    "            raise ValueError(\"Hugging Face API token not found. Set HF_TOKEN environment variable or pass hf_token parameter.\")\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(judge_model, token=hf_token)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            judge_model,\n",
    "            token=hf_token,\n",
    "            device_map=device,  # Use selected device\n",
    "            torch_dtype=torch_dtype  # Use appropriate precision\n",
    "        )\n",
    "        \n",
    "        # Setup pipeline for text generation\n",
    "        evaluator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.1,  # Low temperature for consistent evaluation\n",
    "            device_map=device,  # Use selected device\n",
    "            return_full_text=False  # Return only generated text\n",
    "        )\n",
    "        print(f\"Model {judge_model} loaded successfully on {device}\")\n",
    "        return evaluator\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load Hugging Face model {judge_model}: {str(e)}\")\n",
    "\n",
    "# Modified evaluation function to return raw response\n",
    "def evaluate_llm_response(question: str, ground_truth: str, llm_response: str, \n",
    "                         config_json: str, use_detailed: bool = False, \n",
    "                         evaluation_context: str = \"General knowledge evaluation\",\n",
    "                         hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Evaluate an LLM response against ground truth using Hugging Face model from JSON string config\n",
    "    \n",
    "    Args:\n",
    "        question: The original question asked\n",
    "        ground_truth: The correct/expected answer\n",
    "        llm_response: The LLM's response to evaluate\n",
    "        config_json: JSON string containing judge_model\n",
    "        use_detailed: Whether to use the detailed evaluation prompt\n",
    "        evaluation_context: Context for the evaluation task\n",
    "        hf_token: Hugging Face API token (optional if env var is set)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results and raw LLM response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup Hugging Face evaluator\n",
    "    llm_evaluator = setup_hf_evaluator(config_json=config_json, hf_token=hf_token)\n",
    "    \n",
    "    # Format prompt\n",
    "    if use_detailed:\n",
    "        prompt = detailed_evaluation_prompt.format(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            llm_response=llm_response,\n",
    "            evaluation_context=evaluation_context\n",
    "        )\n",
    "    else:\n",
    "        prompt = evaluation_prompt.format(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            llm_response=llm_response\n",
    "        )\n",
    "    \n",
    "    # Get evaluation from Hugging Face model\n",
    "    try:\n",
    "        evaluation_response = llm_evaluator(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            return_full_text=False\n",
    "        )[0]['generated_text']\n",
    "        print(f\"Raw DLL Response:\\n{evaluation_response}\\n{'-'*50}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Hugging Face model: {e}\")\n",
    "        return {\n",
    "            \"error\": f\"Model inference failed: {str(e)}\",\n",
    "            \"overall_score\": 0.0,\n",
    "            \"raw_response\": None\n",
    "        }\n",
    "    \n",
    "    # Parse the response\n",
    "    parser = EvaluationOutputParser()\n",
    "    evaluation_results = parser.parse(evaluation_response)\n",
    "    \n",
    "    # Include raw response in results\n",
    "    evaluation_results[\"raw_response\"] = evaluation_response\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "# Modified batch evaluation function to include raw response\n",
    "def batch_evaluate_responses(evaluation_data: list, config_json: str, \n",
    "                           use_detailed: bool = False, hf_token: str = None):\n",
    "    \"\"\"\n",
    "    Evaluate multiple LLM responses in batch using Hugging Face model from JSON string config\n",
    "    \n",
    "    Args:\n",
    "        evaluation_data: List of dicts with keys: 'question', 'ground_truth', 'llm_response'\n",
    "        config_json: JSON string containing judge_model\n",
    "        use_detailed: Whether to use detailed evaluation\n",
    "        hf_token: Hugging Face API token\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation results including raw responses\n",
    "    \"\"\"\n",
    "    evaluator = setup_hf_evaluator(config_json=config_json, hf_token=hf_token)\n",
    "    \n",
    "    results = []\n",
    "    for i, data in enumerate(evaluation_data):\n",
    "        print(f\"Evaluating response {i+1}/{len(evaluation_data)}...\")\n",
    "        \n",
    "        result = evaluate_llm_response(\n",
    "            question=data['question'],\n",
    "            ground_truth=data['ground_truth'],\n",
    "            llm_response=data['llm_response'],\n",
    "            config_json=config_json,\n",
    "            use_detailed=use_detailed,\n",
    "            hf_token=hf_token\n",
    "        )\n",
    "        \n",
    "        result['evaluation_id'] = i\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Judge Model Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cf76d-2a2f-4c60-838c-1dceb96296fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON configuration as a string\n",
    "config_json = json.dumps(config)\n",
    "\n",
    "# Example evaluation data\n",
    "evaluation_data = results\n",
    "\n",
    "# Evaluate responses\n",
    "results = batch_evaluate_responses(\n",
    "    evaluation_data=evaluation_data,\n",
    "    config_json=config_json,\n",
    "    use_detailed=False,\n",
    "    hf_token=\"Sample_hf\"  # Replace with your actual HF token or set HF_TOKEN env variable\n",
    ")\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367c3b87-97ae-4812-adda-3e3e7a324269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT RESULTS TO A FILE\n",
    "\n",
    "output_file = \"evaluation_results.json\"\n",
    "try:\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(f\"Successfully saved evaluation results to '{output_file}'\")\n",
    "except IOError as e:\n",
    "    logging.error(f\"Error: Failed to write to '{output_file}': {e}\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error: An unexpected error occurred while saving '{output_file}': {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeff6d4-9482-42e4-a4b4-9a9bb8ddc230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
