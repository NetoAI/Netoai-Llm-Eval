{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402716f5",
   "metadata": {},
   "source": [
    "## LLM Response Generation and Evaluation Framework\n",
    "<img src=\"netoai_logo.png\" alt=\"Alt Text\" width=\"300\"/>\n"
    "\n",
    "\n",
    "This section demonstrates how to generate responses using a selected LLM and evaluate them with a judge model. The input dataset must contain `question` and `answer` columns. The prompt template should include a `<question>` placeholder, which will be dynamically replaced during inference. The generated output is stored in a structured format and later evaluated to assign numerical scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cfca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and logging configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Imports ---\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizer, PreTrainedModel, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# Configure logging to see informative messages\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "print(\"Imports and logging configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc55fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Part 1 - Inference Configuration ---\n",
    "\n",
    "# This config is for the INFERENCE step (generating answers)\n",
    "config = {\n",
    "  \"model_to_be_tested\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "  \"dataset_hf_name\": \"squad\",\n",
    "  \"judge_model\":\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "  \"dataset_split\": \"validation\",\n",
    "  \"question_column_name\": \"question\",\n",
    "  \"prompt_template\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are an expert assistant who provides concise and accurate answers.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Please answer the following question: <question>\"\n",
    "    }\n",
    "  ],\n",
    "  \"num_samples\": 3,\n",
    "  \"max_new_tokens\": 256,\n",
    "  \"output_file\": \"inference_results.json\"\n",
    "}\n",
    "\n",
    "print(\"Inference configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aff7358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Inference Helper Functions ---\n",
    "\n",
    "def load_hf_dataset(name: str, split: str, question_column: str) -> Dataset:\n",
    "    \"\"\"Loads a dataset from Hugging Face Hub and verifies the question column.\"\"\"\n",
    "    logging.info(f\"Loading dataset '{name}' (split: {split})...\")\n",
    "    try:\n",
    "        dataset = load_dataset(name, split=split)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset '{name}': {e}\")\n",
    "\n",
    "    if question_column not in dataset.column_names:\n",
    "        raise ValueError(\n",
    "            f\"Dataset '{name}' does not have a '{question_column}' column. \"\n",
    "            f\"Available columns: {dataset.column_names}\"\n",
    "        )\n",
    "    logging.info(f\"Dataset loaded. Using '{question_column}' as the question column.\")\n",
    "    return dataset\n",
    "\n",
    "def load_model_and_tokenizer(model_name: str) -> Tuple[PreTrainedModel, PreTrainedTokenizer]:\n",
    "    \"\"\"Loads a causal language model and its tokenizer from Hugging Face Hub.\"\"\"\n",
    "    logging.info(f\"Loading model and tokenizer for '{model_name}'...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        logging.info(f\"Model loaded successfully on device: {model.device}\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load model or tokenizer '{model_name}': {e}\")\n",
    "\n",
    "def format_prompt(item: Dict, question_column: str, prompt_template: Any, tokenizer: PreTrainedTokenizer) -> str:\n",
    "    \"\"\"Formats the prompt using the provided template and data item.\"\"\"\n",
    "    try:\n",
    "        question = item[question_column]\n",
    "        if isinstance(prompt_template, str):\n",
    "            return prompt_template.replace(\"<question>\", question)\n",
    "        elif isinstance(prompt_template, list):\n",
    "            messages = [\n",
    "                {\"role\": msg[\"role\"], \"content\": msg[\"content\"].replace(\"<question>\", question)}\n",
    "                for msg in prompt_template\n",
    "            ]\n",
    "            return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported prompt_template type: {type(prompt_template)}\")\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Question column '{question_column}' not found in the data item.\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error formatting prompt: {e}\")\n",
    "\n",
    "def perform_inference(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str, max_new_tokens: int) -> Tuple[List[int], int]:\n",
    "    \"\"\"Performs inference using the model.\"\"\"\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        input_ids_len = inputs.input_ids.shape[1]\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        return generated_ids[0].tolist(), input_ids_len\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logging.error(\"CUDA Out of Memory. Try a smaller model, fewer samples, or quantization.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during model generation: {e}\")\n",
    "        raise e\n",
    "\n",
    "def parse_generation(tokenizer: PreTrainedTokenizer, full_output_ids: List[int], input_length: int) -> str:\n",
    "    \"\"\"Decodes the generated token IDs, excluding the prompt.\"\"\"\n",
    "    output_ids = full_output_ids[input_length:]\n",
    "    response = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "print(\"Inference helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bba5cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 13:54:21 - INFO - Loading model and tokenizer for 'Qwen/Qwen2-0.5B-Instruct'...\n",
      "2025-06-16 13:54:27 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-06-16 13:54:29 - INFO - Model loaded successfully on device: cuda:0\n",
      "2025-06-16 13:54:29 - INFO - Loading dataset 'squad' (split: validation)...\n",
      "Using the latest cached version of the dataset since squad couldn't be found on the Hugging Face Hub\n",
      "2025-06-16 13:55:59 - WARNING - Using the latest cached version of the dataset since squad couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at C:\\Users\\menon\\.cache\\huggingface\\datasets\\squad\\plain_text\\0.0.0\\7b6d24c440a36b6815f21b70d25016731768db1f (last modified on Mon Jun 16 11:40:54 2025).\n",
      "2025-06-16 13:55:59 - WARNING - Found the latest cached dataset configuration 'plain_text' at C:\\Users\\menon\\.cache\\huggingface\\datasets\\squad\\plain_text\\0.0.0\\7b6d24c440a36b6815f21b70d25016731768db1f (last modified on Mon Jun 16 11:40:54 2025).\n",
      "2025-06-16 13:55:59 - INFO - Dataset loaded. Using 'question' as the question column.\n",
      "2025-06-16 13:55:59 - INFO - --- Processing sample 1/3 ---\n",
      "2025-06-16 13:56:02 - INFO - Generated response for sample 1.\n",
      "2025-06-16 13:56:02 - INFO - --- Processing sample 2/3 ---\n",
      "2025-06-16 13:56:04 - INFO - Generated response for sample 2.\n",
      "2025-06-16 13:56:04 - INFO - --- Processing sample 3/3 ---\n",
      "2025-06-16 13:56:07 - INFO - Generated response for sample 3.\n",
      "2025-06-16 13:56:07 - INFO - Saving 3 results to inference_results.json...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference completed successfully! Results saved to inference_results.json\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Inference Pipeline ---\n",
    "\n",
    "results = []\n",
    "try:\n",
    "    # Unpack configuration\n",
    "    model_name = config[\"model_to_be_tested\"]\n",
    "    dataset_name = config[\"dataset_hf_name\"]\n",
    "    dataset_split = config[\"dataset_split\"]\n",
    "    question_column = config[\"question_column_name\"]\n",
    "    prompt_template = config[\"prompt_template\"]\n",
    "    num_samples = config[\"num_samples\"]\n",
    "    max_new_tokens = config[\"max_new_tokens\"]\n",
    "    output_file = Path(config[\"output_file\"])\n",
    "\n",
    "    # Load Model and Dataset\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    dataset = load_hf_dataset(dataset_name, dataset_split, question_column)\n",
    "\n",
    "    # Process Samples\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        logging.info(f\"--- Processing sample {i+1}/{num_samples} ---\")\n",
    "        item = dataset[i]\n",
    "        \n",
    "        prompt = format_prompt(item, question_column, prompt_template, tokenizer)\n",
    "        generated_ids, input_len = perform_inference(model, tokenizer, prompt, max_new_tokens)\n",
    "        llm_response = parse_generation(tokenizer, generated_ids, input_len)\n",
    "        \n",
    "        ground_truth = item.get(\"answers\", {}).get(\"text\", [\"N/A\"])[0]\n",
    "\n",
    "        # SIMPLIFICATION: Save with the key 'ground_truth' to avoid renaming it later.\n",
    "        results.append({\n",
    "            \"question\": item[question_column],\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"llm_response\": llm_response,\n",
    "        })\n",
    "        logging.info(f\"Generated response for sample {i+1}.\")\n",
    "\n",
    "    # Save Results\n",
    "    logging.info(f\"Saving {len(results)} results to {output_file}...\")\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nInference completed successfully! Results saved to {output_file}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during inference: {e}\", exc_info=True)\n",
    "    print(f\"\\nInference pipeline failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87697b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Part 2 - Simplified Evaluation Functions ---\n",
    "\n",
    "class SimpleScoreParser(BaseOutputParser):\n",
    "    \"\"\"A simple parser to extract a single score from the judge's response.\"\"\"\n",
    "    def parse(self, text: str) -> float:\n",
    "        \"\"\"Parses the LLM output to extract only the overall_score.\"\"\"\n",
    "        try:\n",
    "            # Find the JSON block in the model's output\n",
    "            start_idx = text.find('{')\n",
    "            end_idx = text.rfind('}') + 1\n",
    "            json_str = text[start_idx:end_idx]\n",
    "            data = json.loads(json_str)\n",
    "            # Safely get the score, default to 0.0 if not found or not a number\n",
    "            return float(data.get(\"overall_score\", 0.0))\n",
    "        except (json.JSONDecodeError, ValueError, AttributeError):\n",
    "            logging.warning(f\"Could not parse score from response: {text}\")\n",
    "            return 0.0  # Return a default score on any failure\n",
    "\n",
    "# A much simpler and stricter prompt for the judge model\n",
    "simple_evaluation_prompt = PromptTemplate(\n",
    "    input_variables=[\"ground_truth\", \"llm_response\"],\n",
    "    template=\"\"\"You are a strict evaluator. Your only job is to provide a single score.\n",
    "Based on the Ground Truth, score the LLM Response from 0.0 (completely wrong) to 10.0 (perfectly correct).\n",
    "\n",
    "Ground Truth: {ground_truth}\n",
    "LLM Response: {llm_response}\n",
    "\n",
    "You MUST ONLY output a JSON object with a single key \"overall_score\". Do not add any other text.\n",
    "Example: {{\"overall_score\": 8.5}}\n",
    "\n",
    "JSON output:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def setup_hf_evaluator(judge_model_name: str, hf_token: str = None):\n",
    "    \"\"\"Sets up the Hugging Face pipeline for the evaluator model.\"\"\"\n",
    "    logging.info(f\"Setting up judge model: {judge_model_name}\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    try:\n",
    "        return pipeline(\"text-generation\", model=judge_model_name, device=device, token=hf_token)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load judge model '{judge_model_name}': {e}\")\n",
    "\n",
    "def batch_evaluate_simple_score(evaluation_data: list, judge_model_name: str, hf_token: str = None):\n",
    "    \"\"\"Evaluates responses to get a single score for each.\"\"\"\n",
    "    if not evaluation_data:\n",
    "        return []\n",
    "\n",
    "    evaluator = setup_hf_evaluator(judge_model_name, hf_token)\n",
    "    parser = SimpleScoreParser()\n",
    "    final_results = []\n",
    "\n",
    "    for i, item in enumerate(evaluation_data):\n",
    "        logging.info(f\"--- Evaluating item {i+1}/{len(evaluation_data)} ---\")\n",
    "        prompt = simple_evaluation_prompt.format(\n",
    "            ground_truth=item['ground_truth'],\n",
    "            llm_response=item['llm_response']\n",
    "        )\n",
    "        \n",
    "        raw_response = evaluator(prompt, max_new_tokens=50, return_full_text=False)[0]['generated_text']\n",
    "        final_score = parser.parse(raw_response)\n",
    "        \n",
    "        # Construct the simple, final dictionary\n",
    "        final_results.append({\n",
    "            \"question\": item['question'],\n",
    "            \"ground_truth\": item['ground_truth'],\n",
    "            \"llm_response\": item['llm_response'],\n",
    "            \"final_score\": final_score\n",
    "        })\n",
    "            \n",
    "    return final_results\n",
    "\n",
    "print(\"Simplified evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df916fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 7: Final Execution Pipeline ---\n",
    "\n",
    "# 1. Define configuration for the JUDGE model\n",
    "judge_model_name = config[\"judge_model\"]\n",
    "hf_token = \"hf_...\"  # IMPORTANT: Replace with your actual Hugging Face token\n",
    "\n",
    "# 2. Load the results from the inference step\n",
    "def load_json_from_file(file_path: str) -> Union[Dict, List, Any]:\n",
    "    \"\"\"Safely loads data from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "\n",
    "inference_results_file = config.get(\"output_file\", \"inference_results.json\")\n",
    "data_to_evaluate = load_json_from_file(inference_results_file)\n",
    "\n",
    "# 3. Run the simplified batch evaluation\n",
    "if data_to_evaluate:\n",
    "    print(\"\\nStarting simplified batch evaluation...\")\n",
    "    final_scores = batch_evaluate_simple_score(\n",
    "        evaluation_data=data_to_evaluate,\n",
    "        judge_model_name=judge_model_name,\n",
    "        hf_token=hf_token\n",
    "    )\n",
    "\n",
    "    # 4. Save and print the final, simple results\n",
    "    evaluation_output_file = \"final_scores.json\"\n",
    "    with open(evaluation_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_scores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. Results saved to {evaluation_output_file}\")\n",
    "    print(\"\\n--- FINAL SCORES ---\")\n",
    "    print(json.dumps(final_scores, indent=2))\n",
    "else:\n",
    "    print(\"\\nCould not proceed with evaluation. No data loaded from inference file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a713d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
